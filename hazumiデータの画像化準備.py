# -*- coding: utf-8 -*-
"""Hazumiデータの画像化準備[Cuiさん作成のプログラムを理解しながら改良].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12voGHlKpywsvo7hCkHLmY9u6SQntG6al

# Google Driveからのマウント
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 異常検知のためのHazumiデータの画像化

## Kinectデータを発話区間ごとに分割
"""

import pandas as pd
import os

# 今回対象とするユーザの名前(Hazumi)
user_name = "1712F2006" #@param {type:"string"}

# hazumiのversion(1911, 1902など)
hazumi_version = "1712" #@param {type:"string"}

if hazumi_version == "1911":
  # Hazumi1911
  user_list = ["1911F2001", "1911F2002", "1911F3001", "1911F3002", "1911F3003",
              "1911F4001", "1911F4002", "1911F4003", "1911F5001", "1911F5002",
              "1911F6001", "1911F6002", "1911F6003", "1911F7002", "1911M2001", 
              "1911M2002", "1911M2003", "1911M4001", "1911M4002", "1911M5001",
              "1911M5002", "1911M6001", "1911M6002", "1911M6003", "1911M7001", "1911M7002"]
elif hazumi_version == "1902":
  # Hazumi1902
  # dumpfileに1902F4007.csv,1902M4003.csvが含まれていなかった
  user_list = ["1902F2001", "1902F2002", "1902F3001", "1902F3002", "1902F4001",
              "1902F4002", "1902F4003", "1902F4004", "1902F4005", "1902F4006",
              "1902F4008", "1902F4009", "1902F4010", "1902F4011", "1902F6001", 
              "1902F6002", "1902F7001", "1902F7002", "1902F7003", "1902M2001",
              "1902M3001", "1902M4001", "1902M4002", "1902M5001", "1902M5002", 
              "1902M5003", "1902M7001", "1902M7002"]
  # 1902F7001を除外
  user_list = ["1902F2001", "1902F2002", "1902F3001", "1902F3002", "1902F4001",
              "1902F4002", "1902F4003", "1902F4004", "1902F4005", "1902F4006",
              "1902F4008", "1902F4009", "1902F4010", "1902F4011", "1902F6001", 
              "1902F6002", "1902F7002", "1902F7003", "1902M2001", "1902M3001", 
               "1902M4001", "1902M4002", "1902M5001", "1902M5002", "1902M5003",
               "1902M7001", "1902M7002"]
elif hazumi_version == "1712":
  # Hazumi1712
  # dumpfileに1712F4002.csv,1712M5003.csvが含まれていなかった
  user_list = ["1712F2006", "1712F2010", "1712F2018", "1712F2019", "1712F3013",
              "1712F3016", "1712F3030", "1712F4011", "1712F4012", "1712F4017",
              "1712F4027", "1712F5008", "1712F5014", "1712F5026", "1712M2007", 
              "1712M2021", "1712M2024", "1712M2028", "1712M2029", "1712M3001",
              "1712M3005", "1712M3009", "1712M4004", "1712M4015", "1712M4020", 
              "1712M4022", "1712M4025"]
print(user_list)

'''
Kinectの生データの読み込み
'''
bodydata_path = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/Kinect/{}/Body/BodyData.csv".format(hazumi_version, user_name)
bodydata_df = pd.read_csv(bodydata_path)
bodydata_df # DataFrameの確認

'''
Kinectの計測時間(SysTime)の初期値を0に指定
初期値は実験の撮影開始時間

-元の数値はKinect 収録日の午前 0 時が 0 (概要のA 時刻)
'''
# Kinect内部時間の撮影開始を記したAudioTimeStamp.csvを読み込み
audiotimestamp_path = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/Kinect/{}/Audio/AudioTimeStamp.csv".format(hazumi_version, user_name)
audiotimestamp_df = pd.read_csv(audiotimestamp_path)
print(audiotimestamp_df.iloc[0,0])

# SysTime(Kinect内部時間)のスタート時間を0に補正
bodydata_df['SysTime'] = bodydata_df['SysTime'] - audiotimestamp_df.iloc[0,0]# bodydata_df['SysTime'][0]
bodydata_df # DataFrameの確認

'''
Hazumi dumpfileの読み込み
機械学習のための1発話ごとの統計量を格納したファイル
列ごとに発話の開始と終了が記載

実験参加者ビデオデータの時刻表現(概要のC時刻)
・start(exchange) : システム発話の開始時刻
・end(system)：システム発話の終了時刻
・end(exchange) ：次のシステム発話の開始時刻

Kinect データの時刻表現：Kinect の収録を開始した時点を 0 とした場合の時刻(概要のB時刻)
・kinectstart(exchange)：システム発話の開始時刻
・kinectend(system)：システム発話の終了時刻
・kinectend(exchange)：次のシステム発話の開始時刻
'''
dumpfile_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/dumpfiles/{}.csv".format(hazumi_version, user_name)
dumpfile_df = pd.read_csv(dumpfile_url)
dumpfile_df

def df_split_per_utterance(start_dumpfile, end_dumpfile):
    '''
    入力:
        start_dumpfile: dumpfile(csv)に記載された発話区間の開始秒数
        end_dumpfile: dumpfile(csv)に記載された発話区間の終了秒数
    出力:
        separated_bodydata: Kinect内部時間(開始時刻=0に調整済み)に相当する開始・終了秒数の区間にあるKinectの全フレーム情報
    
    変数:
        start_dumpfile: dumpfile(csv)に記載された発話区間の開始秒数, int
        end_dumpfile: dumpfile(csv)に記載された発話区間の終了秒数, int
        bodydata_systime: Kinectの内部時間の時刻, dataseries
        start_diff: Kinectの内部時間をdumpfileの発話開始時間で引いたもの(結果、0に近いものが最も発話開始に該当するフレーム/行)
        end_diff: Kinectの内部時間をdumpfileの発話終了時間で引いたもの(結果、0に近いものが最も発話開始に該当するフレーム/行)
        start_systime: start_diffの最小値の行におけるKinect内部時間
        end_systime: end_diffの最小値の行におけるKinect内部時間
    '''
    
    # Kinect生データの時間獲得(開始時刻を0秒に調整済み)
    ## 正確にはAudioStanpの1行1列目がKinectの収録開始時刻らしい
    bodydata_systime = bodydata_df["SysTime"]
    
    # systimeにある全ての時間を該当発話(関数で指定の)開始・終了時間で引く
    start_diff = abs(bodydata_systime - start_dumpfile)
    end_diff = abs(bodydata_systime - end_dumpfile)

    # start_diff / end_diffで最も0に近い値が、該当の発話でのKinectの開始時刻と終了時刻
    ## idxminで差が最小となる行番号を取得 -> Kinectの内部時間での該当する行の時間を取得
    start_systime = bodydata_systime.iloc[start_diff.idxmin()]
    end_systime = bodydata_systime.iloc[end_diff.idxmin()]
    print(start_systime, end_systime)
    print(start_diff.idxmin(), end_diff.idxmin())
    # Kinect内部時間における発話開始・終了を「行」で指定、発話区間で区切ったDataFrameを作成
    separated_bodydata = bodydata_df.iloc[start_diff.idxmin():end_diff.idxmin(), :]
    
    return separated_bodydata

# 保存先のフォルダの作成
path = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/".format(hazumi_version)
try:
    separated_kinect_dir = os.path.join(path, 'separated_kinect')
    os.mkdir(separated_kinect_dir)
except FileExistsError:
    print(separated_kinect_dir + "は作成済み")
try:
    user_name_dir = separated_kinect_dir + "/" + user_name
    os.mkdir(user_name_dir)
except FileExistsError:
    print(user_name_dir + "は作成済み")

# dumpfileの行数(発話数)の獲得
num_utterance =  dumpfile_df.shape[0]

# 発話数分のcsvを作成
for utterance_idx in range(0, num_utterance):
    # kinectの撮影開始時間が基準
    utterance_start = int(dumpfile_df.iloc[utterance_idx:utterance_idx+1, 0:]["kinectend(system)[ms]"])
    utterance_end = int(dumpfile_df.iloc[utterance_idx:utterance_idx+1, 0:]["kinectend(exchange)[ms]"])

    # 時間合わせの関数
    ## 入力：dumpfileの開始・終了時間 / 出力：Kinect内部時間の開始・終了時間で区切られたDataFrame
    separated_bodydata = df_split_per_utterance(utterance_start, utterance_end)

    # 使用する特徴のみを抽出(省略可能：データ量の削減のため -> あまり削減にはならなかった)
    separated_bodydata = separated_bodydata.loc[:, "Head_X":"FootRight_Z"]

    # 指定されたフォルダに保存
    save_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/separated_kinect/{}/utterance{}.csv".format(hazumi_version, user_name, str(utterance_idx))
    separated_bodydata.to_csv(save_url)
    print(utterance_idx)

"""## 発話区間ごとの時系列情報を画像化"""

import csv
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from scipy import misc
import cv2
import os
from sklearn import preprocessing

# 外れ値を削除する関数
def drop_outlier(df):
  for i, col in df.iteritems():
    # 外れ値の基準点: 分位数95％の値
    #q = col.quantile(0.95)
    q = col.quantile(0.99)
    print(q)

    #範囲から外れている値を除く
    col[col > q] = None

# 今回対象とするユーザの名前(Hazumi)
#user_name = "1911F2001" #@param {type:"string"}

# hazumiのversion(1911, 1902など)
#hazumi_version = "1911" #@param {type:"string"}

# 保存先のフォルダの作成
path = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/".format(hazumi_version)
try:
    png_dir = os.path.join(path, 'png')
    os.mkdir(png_dir)
except FileExistsError:
    print(png_dir + "は作成済み")
try:
    user_name_dir = png_dir + "/" + user_name
    os.mkdir(user_name_dir)
except FileExistsError:
    print(user_name_dir + "は作成済み")

'''
入力：
    utterance{idx}.csv: 発話ごとに区切られたKinectデータ(bodydata.csv)
    (例：utterance1.csv)
出力：
    utterance{idx}.png: 発話ごとに区切られたKinectデータの行列を画像として表現 
    (例：uttearnce1.png)
'''

# 発話の数だけデータを獲得
folder_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/separated_kinect/{}".format(hazumi_version, user_name)
for csv_idx, csv_file in enumerate(os.listdir(folder_url)):
    file_url = folder_url + "/{}".format(csv_file)
    separated_bodydata = pd.read_csv(file_url)

    # 使用する特徴のみを抽出
    separated_bodydata = separated_bodydata.loc[:, "Head_X":"FootRight_Z"]
    
    # フレーム間差分の計算
    separated_bodydata_frame_diff = separated_bodydata.diff()

    # 一番最初の行のNaN値を削除
    separated_bodydata_frame_diff = separated_bodydata_frame_diff.drop(separated_bodydata_frame_diff.index[0]).reset_index(drop=True)

    # 絶対値に変換 (数値はフレーム間差分なので、+，-も「動いた量」になるので変わらない)
    separated_bodydata_frame_diff = separated_bodydata_frame_diff.abs()

    # 外れ値の除去
    drop_outlier(separated_bodydata_frame_diff)
    ## Nan(外れ値と判定されたもの)の数をカウント
    print(separated_bodydata_frame_diff.isnull().sum())
    # Nan(外れ値と判定されたもの)の部分を線形補間
    separated_bodydata_frame_diff.interpolate(inplace=True, axis = 0)

    # 正規化(0 ~ 1)
    separated_bodydata_frame_diff_norm = (separated_bodydata_frame_diff - separated_bodydata_frame_diff.min(axis = 0)) / (separated_bodydata_frame_diff.max(axis = 0) - separated_bodydata_frame_diff.min(axis = 0))

    # Dataframe -> Numpy.array -> PIL Image
    ## numpyに変換
    separated_bodydata_frame_diff_norm_array = separated_bodydata_frame_diff_norm.to_numpy()

    ## 画像化のために（0−255）への変換
    separated_bodydata_frame_diff_norm_array = separated_bodydata_frame_diff_norm_array * 255

    ## numpyを画像化(モノクロ)
    pil_img = Image.fromarray(separated_bodydata_frame_diff_norm_array.astype(np.uint8))

    # 画像のリサイズ
    ## 特徴量の数が75
    resized_pil_img = pil_img.resize((75, 75))

    save_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/png/{}".format(hazumi_version, user_name)
    resized_pil_img.save("{}/utterance{}.png".format(save_url, csv_idx))

"""## ラベルを二値に変換"""

import csv
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from scipy import misc
import cv2
import os
import shutil

# 今回対象とするユーザの名前(Hazumi)
#user_name = "1911F2001" #@param {type:"string"}

# hazumiのversion(1911, 1902など)
#hazumi_version = "1911" #@param {type:"string"}

'''
Hazumi dumpfileの読み込み
機械学習のための1発話ごとの統計量を格納したファイル
列ごとに発話の開始と終了が記載

実験参加者ビデオデータの時刻表現(概要のC時刻)
・start(exchange) : システム発話の開始時刻
・end(system)：システム発話の終了時刻
・end(exchange) ：次のシステム発話の開始時刻

Kinect データの時刻表現：Kinect の収録を開始した時点を 0 とした場合の時刻(概要のB時刻)
・kinectstart(exchange)：システム発話の開始時刻
・kinectend(system)：システム発話の終了時刻
・kinectend(exchange)：次のシステム発話の開始時刻
'''
dumpfile_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/dumpfiles/{}.csv".format(hazumi_version, user_name)
dumpfile_df = pd.read_csv(dumpfile_url)
dumpfile_df

# 5人の他者評価を平均して、2値ラベルに
'''
入力:
    TS1 ~ TS5: dumpfileに格納されている一発話ごとの他者からのアノテーションデータ(ネガティブ[1] - ポジティブ[7])
    TC1 ~ TC5: dumpfileに格納されている一発話ごとの他者からのアノテーションデータ(話題を変えたい[1] - 話題を継続したい[7])
出力:
    5人の他者評価を平均して、各発話に対して2値ラベルを付与
'''

# 5人の他者評価部分を切り取り
TS = dumpfile_df.loc[:, "TS1" : "TS5"] # ネガティブポジティブ
TC = dumpfile_df.loc[:, "TC1" : "TC5"] # 話題継続

# 5人の他者評価を平均
TS_mean = TS.mean(axis = "columns") # 行ごと（発話ごと）に5人の評定を平均
TC_mean = TC.mean(axis = "columns") # 行ごと（発話ごと）に5人の評定を平均

# 閾値(theshold)を基に、2値ラベルに変換
## 平均値が4より大きいなら1, 未満なら0
TS_binary = TS_mean.apply(lambda x :1 if x > 4 else 0) 
TC_binary = TC_mean.apply(lambda x :1 if x > 4 else 0)

TS_binary.head(-1)
# print(TC_binary)

"""### 生成した画像を対応するラベル名が記載されたフォルダに格納

フォルダ名
*   label_1
*   label_0
"""

# 保存先のフォルダの作成
path = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/".format(hazumi_version)
try:
    png_dir = os.path.join(path, 'png')
    os.mkdir(png_dir)
except FileExistsError:
    print(png_dir + "は作成済み")
try:
    user_name_dir = png_dir + "/" + user_name
    os.mkdir(user_name_dir)
except FileExistsError:
    print(user_name_dir + "は作成済み")

# feature_folder
try:
    feature_dir = user_name_dir + "/frame_sub"
    os.mkdir(feature_dir)
except FileExistsError:
    print(feature_dir + "は作成済み")

# 各ラベルに対応する画像を保存するフォルダの作成
try:
    label1_dir = feature_dir + "/label_1"
    os.mkdir(label1_dir)
except FileExistsError:
    print(label1_dir + "は作成済み")

try:
    label0_dir = feature_dir + "/label_0"
    os.mkdir(label0_dir)
except FileExistsError:
    print(label0_dir + "は作成済み")

# TS_binary(DataFrame)の全発話分を実行
for img_idx in range(TS_binary.shape[0]):
    # 既に保存されている画像を参照
    original_img_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/png/{}/utterance{}.png".format(hazumi_version, user_name, img_idx)

    # 移動先のURLを指定
    ## ラベル1: TS_binaryの値が1のとき
    if TS_binary[img_idx] == 1:    
        moved_img_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/png/{}/frame_sub/label_1/utterance{}.png".format(hazumi_version, user_name, img_idx)
    
    ## ラベル０: TS_binaryの値が0のとき
    elif TS_binary[img_idx] == 0:
        moved_img_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/png/{}/frame_sub/label_0/utterance{}.png".format(hazumi_version, user_name, img_idx)

    # 画像を移動
    shutil.move(original_img_url, moved_img_url)
print("各画像の対応ラベルへの割り振り完了")

"""## テストと訓練データに振り分け

画像データを学習・評価に分割
*   入力：ラベルが分割された画像データ群
*   出力：学習・評価フォルダにそれぞれのラベルの画像データ
（今回は異常検知のため、異常ラベルは学習に含まない）
"""

import sys
import csv
import glob
import os
import re
import random
import shutil
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import seaborn as sns

## 画像ディレクトリをtrain_test_split
def image_dir_train_test_sprit(original_dir, base_dir, seed, train_size=0.8):
    '''
    画像データをトレインデータとテストデータにシャッフルして分割します。フォルダもなければ作成します。

    parameter
    ------------
    original_dir: str
      オリジナルデータフォルダのパス その下に各クラスのフォルダがある
    base_dir: str
      分けたデータを格納するフォルダのパス　そこにフォルダが作られます
    train_size: float
      トレインデータの割合
    '''

    try:
        os.mkdir(base_dir)
    except FileExistsError:
        print(base_dir + "は作成済み")

    #クラス分のフォルダ名の取得
    dir_lists = os.listdir(original_dir)
    dir_lists = [f for f in dir_lists if os.path.isdir(os.path.join(original_dir, f))]
    original_dir_path = [os.path.join(original_dir, p) for p in dir_lists]

    num_class = len(dir_lists)

    # フォルダの作成(トレインとバリデーション)
    try:
        train_dir = os.path.join(base_dir, 'train')
        os.mkdir(train_dir)
    except FileExistsError:
        print(train_dir + "は作成済み")

    try:
        validation_dir = os.path.join(base_dir, 'validation')
        os.mkdir(validation_dir)
    except FileExistsError:
        print(validation_dir + "は作成済み")

    #クラスフォルダの作成
    train_dir_path_lists = []
    val_dir_path_lists = []
    for D in dir_lists:
        train_class_dir_path = os.path.join(train_dir, D)
        try:
            os.mkdir(train_class_dir_path)
        except FileExistsError:
            print(train_class_dir_path + "は作成済み")
        train_dir_path_lists += [train_class_dir_path]
        val_class_dir_path = os.path.join(validation_dir, D)
        try:
            os.mkdir(val_class_dir_path)
        except FileExistsError:
            print(val_class_dir_path + "は作成済み")
        val_dir_path_lists += [val_class_dir_path]


    #元データをシャッフルしたものを上で作ったフォルダにコピーします。
    #ファイル名を取得してシャッフル
    random.seed(seed) # Seedでランダムな割り振りを固定
    for i,path in enumerate(original_dir_path):
        files_class = os.listdir(path)
        random.shuffle(files_class)
        # 分割地点のインデックスを取得
        num_bunkatu = int(len(files_class) * train_size)
        #トレインへファイルをコピー
        for fname in files_class[:num_bunkatu]:
            src = os.path.join(path, fname)
            dst = os.path.join(train_dir_path_lists[i], fname)
            shutil.copyfile(src, dst)
        #valへファイルをコピー
        for fname in files_class[num_bunkatu:]:
            src = os.path.join(path, fname)
            dst = os.path.join(val_dir_path_lists[i], fname)
            shutil.copyfile(src, dst)
        print(path + "コピー完了")

main_folder_name = "Hazumi/Hazumi{}/png".format(hazumi_version)
main_folder_dir = "/content/drive/My Drive/Colab Notebooks/{}".format(main_folder_name)

irregular_label = "label_0" #@param {type:"string"}
feature_folder_name = "frame_sub"#@param {type:"string"}

train_size = 0.7
seed =  1#@param {type:"integer"}

#for file_idx in range(0,len(user_list)):
#original_dir = main_folder_dir + "/" + user_list[file_idx]

original_dir = main_folder_dir + "/" + user_name
# フォルダの作成
experiment_dir = "{}/experiment".format(original_dir)
try:
    os.mkdir(experiment_dir)
except FileExistsError:
    print(experiment_dir + "は作成済み")

# 画像が格納されているフォルダ
image_dir = "{}/{}".format(original_dir, feature_folder_name)

# テストと訓練データ分割後の画像を格納するフォルダ
base_dir = "{}/experiment/splited_test_{}_seed_{}".format(original_dir, irregular_label, str(seed))

# テストと訓練データの分割
image_dir_train_test_sprit(image_dir, base_dir, seed, train_size)

original_dir = base_dir + "/train/{}/".format(irregular_label)
output_dir = base_dir + "/validation/{}/".format(irregular_label)

files_names = os.listdir(original_dir)
for fname in files_names:
    src = os.path.join(original_dir, fname)
    dst = os.path.join(output_dir, fname)
    shutil.copyfile(src, dst)
    os.remove(src)
    print(src + "から" + dst + "をコピー完了")

print("分割終了")



"""## [ここまでの処理まとめ(全員一括処理version)] 発話区切り -> テストと訓練画像データ振り分け

全てのユーザのデータ格納が必須なため、あまり使わないかもしれない
"""

import pandas as pd
import os
import csv
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from scipy import misc
import cv2
from sklearn import preprocessing
from PIL import Image
import matplotlib.pyplot as plt
import shutil
import sys
import glob
import re
import random
import seaborn as sns

# 外れ値を削除する関数
def drop_outlier(df):
  for i, col in df.iteritems():
    # 外れ値の基準点: 分位数95％の値
    #q = col.quantile(0.95)
    q = col.quantile(0.99)
    #print(q)

    #範囲から外れている値を除く
    col[col > q] = None

# hazumiのversion(1911, 1902など)
hazumi_version = "1712" #@param {type:"string"}

if hazumi_version == "1911":
  # Hazumi1911
  user_list = ["1911F2001", "1911F2002", "1911F3001", "1911F3002", "1911F3003",
              "1911F4001", "1911F4002", "1911F4003", "1911F5001", "1911F5002",
              "1911F6001", "1911F6002", "1911F6003", "1911F7002", "1911M2001", 
              "1911M2002", "1911M2003", "1911M4001", "1911M4002", "1911M5001",
              "1911M5002", "1911M6001", "1911M6002", "1911M6003", "1911M7001", "1911M7002"]
elif hazumi_version == "1902":
  # Hazumi1902
  # dumpfileに1902F4007.csv,1902M4003.csvが含まれていなかった
  user_list = ["1902F2001", "1902F2002", "1902F3001", "1902F3002", "1902F4001",
              "1902F4002", "1902F4003", "1902F4004", "1902F4005", "1902F4006",
              "1902F4008", "1902F4009", "1902F4010", "1902F4011", "1902F6001", 
              "1902F6002", "1902F7001", "1902F7002", "1902F7003", "1902M2001",
              "1902M3001", "1902M4001", "1902M4002", "1902M5001", "1902M5002", 
              "1902M5003", "1902M7001", "1902M7002"]
  # 1902F7001を除外
  user_list = ["1902F2001", "1902F2002", "1902F3001", "1902F3002", "1902F4001",
              "1902F4002", "1902F4003", "1902F4004", "1902F4005", "1902F4006",
              "1902F4008", "1902F4009", "1902F4010", "1902F4011", "1902F6001", 
              "1902F6002", "1902F7002", "1902F7003", "1902M2001", "1902M3001", 
               "1902M4001", "1902M4002", "1902M5001", "1902M5002", "1902M5003",
               "1902M7001", "1902M7002"]
elif hazumi_version == "1712":
  # Hazumi1712
  # dumpfileに1712F4002.csv,1712M5003.csvが含まれていなかった
  user_list = ["1712F2006", "1712F2010", "1712F2018", "1712F2019", "1712F3013",
              "1712F3016", "1712F3030", "1712F4011", "1712F4012", "1712F4017",
              "1712F4027", "1712F5008", "1712F5014", "1712F5026", "1712M2007", 
              "1712M2021", "1712M2024", "1712M2028", "1712M2029", "1712M3001",
              "1712M3005", "1712M3009", "1712M4004", "1712M4015", "1712M4020", 
              "1712M4022", "1712M4025"]
print(user_list)

for user_idx in range(0, len(user_list)):
  # 今回対象とするユーザの名前(Hazumi)
  user_name = user_list[user_idx]

  '''
  Kinectの生データの読み込み
  '''
  bodydata_path = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/Kinect/{}/Body/BodyData.csv".format(hazumi_version, user_name)
  bodydata_df = pd.read_csv(bodydata_path)

  '''
  Kinectの計測時間(SysTime)の初期値を0に指定
  初期値は実験の撮影開始時間

  -元の数値はKinect 収録日の午前 0 時が 0 (概要のA 時刻)
  '''
  # Kinect内部時間の撮影開始を記したAudioTimeStamp.csvを読み込み
  audiotimestamp_path = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/Kinect/{}/Audio/AudioTimeStamp.csv".format(hazumi_version, user_name)
  audiotimestamp_df = pd.read_csv(audiotimestamp_path)
  print(audiotimestamp_df.iloc[0,0])

  # SysTime(Kinect内部時間)のスタート時間を0に補正
  bodydata_df['SysTime'] = bodydata_df['SysTime'] - audiotimestamp_df.iloc[0,0]# bodydata_df['SysTime'][0]

  '''
  Hazumi dumpfileの読み込み
  機械学習のための1発話ごとの統計量を格納したファイル
  列ごとに発話の開始と終了が記載

  実験参加者ビデオデータの時刻表現(概要のC時刻)
  ・start(exchange) : システム発話の開始時刻
  ・end(system)：システム発話の終了時刻
  ・end(exchange) ：次のシステム発話の開始時刻

  Kinect データの時刻表現：Kinect の収録を開始した時点を 0 とした場合の時刻(概要のB時刻)
  ・kinectstart(exchange)：システム発話の開始時刻
  ・kinectend(system)：システム発話の終了時刻
  ・kinectend(exchange)：次のシステム発話の開始時刻
  '''
  dumpfile_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/dumpfiles/{}.csv".format(hazumi_version, user_name)
  dumpfile_df = pd.read_csv(dumpfile_url)

  def df_split_per_utterance(start_dumpfile, end_dumpfile):
      '''
      入力:
          start_dumpfile: dumpfile(csv)に記載された発話区間の開始秒数
          end_dumpfile: dumpfile(csv)に記載された発話区間の終了秒数
      出力:
          separated_bodydata: Kinect内部時間(開始時刻=0に調整済み)に相当する開始・終了秒数の区間にあるKinectの全フレーム情報
      
      変数:
          start_dumpfile: dumpfile(csv)に記載された発話区間の開始秒数, int
          end_dumpfile: dumpfile(csv)に記載された発話区間の終了秒数, int
          bodydata_systime: Kinectの内部時間の時刻, dataseries
          start_diff: Kinectの内部時間をdumpfileの発話開始時間で引いたもの(結果、0に近いものが最も発話開始に該当するフレーム/行)
          end_diff: Kinectの内部時間をdumpfileの発話終了時間で引いたもの(結果、0に近いものが最も発話開始に該当するフレーム/行)
          start_systime: start_diffの最小値の行におけるKinect内部時間
          end_systime: end_diffの最小値の行におけるKinect内部時間
      '''
      
      # Kinect生データの時間獲得(開始時刻を0秒に調整済み)
      ## 正確にはAudioStanpの1行1列目がKinectの収録開始時刻らしい
      bodydata_systime = bodydata_df["SysTime"]
      
      # systimeにある全ての時間を該当発話(関数で指定の)開始・終了時間で引く
      start_diff = abs(bodydata_systime - start_dumpfile)
      end_diff = abs(bodydata_systime - end_dumpfile)

      # start_diff / end_diffで最も0に近い値が、該当の発話でのKinectの開始時刻と終了時刻
      ## idxminで差が最小となる行番号を取得 -> Kinectの内部時間での該当する行の時間を取得
      start_systime = bodydata_systime.iloc[start_diff.idxmin()]
      end_systime = bodydata_systime.iloc[end_diff.idxmin()]

      # Kinect内部時間における発話開始・終了を「行」で指定、発話区間で区切ったDataFrameを作成
      separated_bodydata = bodydata_df.iloc[start_diff.idxmin():end_diff.idxmin(), :]
      
      return separated_bodydata

  # 保存先のフォルダの作成
  path = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/".format(hazumi_version)
  try:
      separated_kinect_dir = os.path.join(path, 'separated_kinect')
      os.mkdir(separated_kinect_dir)
  except FileExistsError:
      print(separated_kinect_dir + "は作成済み")
  try:
      user_name_dir = separated_kinect_dir + "/" + user_name
      os.mkdir(user_name_dir)
  except FileExistsError:
      print(user_name_dir + "は作成済み")

  # dumpfileの行数(発話数)の獲得
  num_utterance =  dumpfile_df.shape[0]

  # 発話数分のcsvを作成
  for utterance_idx in range(0, num_utterance):
      # kinectの撮影開始時間が基準
      utterance_start = int(dumpfile_df.iloc[utterance_idx:utterance_idx+1, 0:]["kinectend(system)[ms]"])
      utterance_end = int(dumpfile_df.iloc[utterance_idx:utterance_idx+1, 0:]["kinectend(exchange)[ms]"])

      # 時間合わせの関数
      ## 入力：dumpfileの開始・終了時間 / 出力：Kinect内部時間の開始・終了時間で区切られたDataFrame
      separated_bodydata = df_split_per_utterance(utterance_start, utterance_end)

      # 使用する特徴のみを抽出(省略可能：データ量の削減のため -> あまり削減にはならなかった)
      separated_bodydata = separated_bodydata.loc[:, "Head_X":"FootRight_Z"]

      # 指定されたフォルダに保存
      save_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/separated_kinect/{}/utterance{}.csv".format(hazumi_version, user_name, str(utterance_idx))
      separated_bodydata.to_csv(save_url)


  #####################################################################################################
  '''
  画像化
  '''
  # 保存先のフォルダの作成
  path = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/".format(hazumi_version)
  try:
      png_dir = os.path.join(path, 'png')
      os.mkdir(png_dir)
  except FileExistsError:
      print(png_dir + "は作成済み")
  try:
      user_name_dir = png_dir + "/" + user_name
      os.mkdir(user_name_dir)
  except FileExistsError:
      print(user_name_dir + "は作成済み")

  '''
  入力：
      utterance{idx}.csv: 発話ごとに区切られたKinectデータ(bodydata.csv)
      (例：utterance1.csv)
  出力：
      utterance{idx}.png: 発話ごとに区切られたKinectデータの行列を画像として表現 
      (例：uttearnce1.png)
  '''

  # 発話の数だけデータを獲得
  folder_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/separated_kinect/{}".format(hazumi_version, user_name)
  for csv_idx, csv_file in enumerate(os.listdir(folder_url)):
      file_url = folder_url + "/{}".format(csv_file)
      separated_bodydata = pd.read_csv(file_url)

      # 使用する特徴のみを抽出
      separated_bodydata = separated_bodydata.loc[:, "Head_X":"FootRight_Z"]
      
      # フレーム間差分の計算
      separated_bodydata_frame_diff = separated_bodydata.diff()

      # 一番最初の行のNaN値を削除
      separated_bodydata_frame_diff = separated_bodydata_frame_diff.drop(separated_bodydata_frame_diff.index[0]).reset_index(drop=True)

      # 絶対値に変換 (数値はフレーム間差分なので、+，-も「動いた量」になるので変わらない)
      separated_bodydata_frame_diff = separated_bodydata_frame_diff.abs()

      # 外れ値の除去
      drop_outlier(separated_bodydata_frame_diff)
      ## Nan(外れ値と判定されたもの)の数をカウント
      print(separated_bodydata_frame_diff.isnull().sum())
      # Nan(外れ値と判定されたもの)の部分を線形補間
      separated_bodydata_frame_diff.interpolate(inplace=True, axis = 0)

      # 正規化(0 ~ 1)
      separated_bodydata_frame_diff_norm = (separated_bodydata_frame_diff - separated_bodydata_frame_diff.min(axis = 0)) / (separated_bodydata_frame_diff.max(axis = 0) - separated_bodydata_frame_diff.min(axis = 0))

      # Dataframe -> Numpy.array -> PIL Image
      ## numpyに変換
      separated_bodydata_frame_diff_norm_array = separated_bodydata_frame_diff_norm.to_numpy()

      ## 画像化のために（0−255）への変換
      separated_bodydata_frame_diff_norm_array = separated_bodydata_frame_diff_norm_array * 255

      ## numpyを画像化(モノクロ)
      pil_img = Image.fromarray(separated_bodydata_frame_diff_norm_array.astype(np.uint8))

      # 画像のリサイズ
      ## 特徴量の数が75
      resized_pil_img = pil_img.resize((75, 75))

      save_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/png/{}".format(hazumi_version, user_name)
      resized_pil_img.save("{}/utterance{}.png".format(save_url, csv_idx))

  ######################################################################################
  '''
  ラベルの二値化
  '''
  # 5人の他者評価を平均して、2値ラベルに
  '''
  入力:
      TS1 ~ TS5: dumpfileに格納されている一発話ごとの他者からのアノテーションデータ(ネガティブ[1] - ポジティブ[7])
      TC1 ~ TC5: dumpfileに格納されている一発話ごとの他者からのアノテーションデータ(話題を変えたい[1] - 話題を継続したい[7])
  出力:
      5人の他者評価を平均して、各発話に対して2値ラベルを付与
  '''

  # 5人の他者評価部分を切り取り
  TS = dumpfile_df.loc[:, "TS1" : "TS5"] # ネガティブポジティブ
  TC = dumpfile_df.loc[:, "TC1" : "TC5"] # 話題継続

  # 5人の他者評価を平均
  TS_mean = TS.mean(axis = "columns") # 行ごと（発話ごと）に5人の評定を平均
  TC_mean = TC.mean(axis = "columns") # 行ごと（発話ごと）に5人の評定を平均

  # 閾値(theshold)を基に、2値ラベルに変換
  ## 平均値が4より大きいなら1, 未満なら0
  TS_binary = TS_mean.apply(lambda x :1 if x > 4 else 0) 
  TC_binary = TC_mean.apply(lambda x :1 if x > 4 else 0)

  '''
  生成した画像を対応するラベル名が記載されたフォルダに格納
  '''
  # 保存先のフォルダの作成
  path = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/".format(hazumi_version)
  try:
      png_dir = os.path.join(path, 'png')
      os.mkdir(png_dir)
  except FileExistsError:
      print(png_dir + "は作成済み")
  try:
      user_name_dir = png_dir + "/" + user_name
      os.mkdir(user_name_dir)
  except FileExistsError:
      print(user_name_dir + "は作成済み")

  # feature_folder
  try:
      feature_dir = user_name_dir + "/frame_sub"
      os.mkdir(feature_dir)
  except FileExistsError:
      print(feature_dir + "は作成済み")

  # 各ラベルに対応する画像を保存するフォルダの作成
  try:
      label1_dir = feature_dir + "/label_1"
      os.mkdir(label1_dir)
  except FileExistsError:
      print(label1_dir + "は作成済み")

  try:
      label0_dir = feature_dir + "/label_0"
      os.mkdir(label0_dir)
  except FileExistsError:
      print(label0_dir + "は作成済み")

  # TS_binary(DataFrame)の全発話分を実行
  for img_idx in range(TS_binary.shape[0]):
      # 既に保存されている画像を参照
      original_img_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/png/{}/utterance{}.png".format(hazumi_version, user_name, img_idx)

      # 移動先のURLを指定
      ## ラベル1: TS_binaryの値が1のとき
      if TS_binary[img_idx] == 1:    
          moved_img_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/png/{}/frame_sub/label_1/utterance{}.png".format(hazumi_version, user_name, img_idx)
      
      ## ラベル０: TS_binaryの値が0のとき
      elif TS_binary[img_idx] == 0:
          moved_img_url = "/content/drive/My Drive/Colab Notebooks/Hazumi/Hazumi{}/png/{}/frame_sub/label_0/utterance{}.png".format(hazumi_version, user_name, img_idx)

      # 画像を移動
      shutil.move(original_img_url, moved_img_url)
  print("各画像の対応ラベルへの割り振り完了")

  '''
  テストと訓練データに振り分け
  '''
  
  ## 画像ディレクトリをtrain_test_split
  def image_dir_train_test_sprit(original_dir, base_dir, seed, train_size=0.8):
      '''
      画像データをトレインデータとテストデータにシャッフルして分割します。フォルダもなければ作成します。

      parameter
      ------------
      original_dir: str
        オリジナルデータフォルダのパス その下に各クラスのフォルダがある
      base_dir: str
        分けたデータを格納するフォルダのパス　そこにフォルダが作られます
      train_size: float
        トレインデータの割合
      '''

      try:
          os.mkdir(base_dir)
      except FileExistsError:
          print(base_dir + "は作成済み")

      #クラス分のフォルダ名の取得
      dir_lists = os.listdir(original_dir)
      dir_lists = [f for f in dir_lists if os.path.isdir(os.path.join(original_dir, f))]
      original_dir_path = [os.path.join(original_dir, p) for p in dir_lists]

      num_class = len(dir_lists)

      # フォルダの作成(トレインとバリデーション)
      try:
          train_dir = os.path.join(base_dir, 'train')
          os.mkdir(train_dir)
      except FileExistsError:
          print(train_dir + "は作成済み")

      try:
          validation_dir = os.path.join(base_dir, 'validation')
          os.mkdir(validation_dir)
      except FileExistsError:
          print(validation_dir + "は作成済み")

      #クラスフォルダの作成
      train_dir_path_lists = []
      val_dir_path_lists = []
      for D in dir_lists:
          train_class_dir_path = os.path.join(train_dir, D)
          try:
              os.mkdir(train_class_dir_path)
          except FileExistsError:
              print(train_class_dir_path + "は作成済み")
          train_dir_path_lists += [train_class_dir_path]
          val_class_dir_path = os.path.join(validation_dir, D)
          try:
              os.mkdir(val_class_dir_path)
          except FileExistsError:
              print(val_class_dir_path + "は作成済み")
          val_dir_path_lists += [val_class_dir_path]


      #元データをシャッフルしたものを上で作ったフォルダにコピーします。
      #ファイル名を取得してシャッフル
      random.seed(seed) # Seedでランダムな割り振りを固定
      for i,path in enumerate(original_dir_path):
          files_class = os.listdir(path)
          random.shuffle(files_class)
          # 分割地点のインデックスを取得
          num_bunkatu = int(len(files_class) * train_size)
          #トレインへファイルをコピー
          for fname in files_class[:num_bunkatu]:
              src = os.path.join(path, fname)
              dst = os.path.join(train_dir_path_lists[i], fname)
              shutil.copyfile(src, dst)
          #valへファイルをコピー
          for fname in files_class[num_bunkatu:]:
              src = os.path.join(path, fname)
              dst = os.path.join(val_dir_path_lists[i], fname)
              shutil.copyfile(src, dst)
          print(path + "コピー完了")

  main_folder_name = "Hazumi/Hazumi{}/png".format(hazumi_version)
  main_folder_dir = "/content/drive/My Drive/Colab Notebooks/{}".format(main_folder_name)

  irregular_label = "label_0"
  feature_folder_name = "frame_sub"

  train_size = 0.7
  seed =  1

  original_dir = main_folder_dir + "/" + user_name

  # フォルダの作成
  experiment_dir = "{}/experiment".format(original_dir)
  try:
      os.mkdir(experiment_dir)
  except FileExistsError:
      print(experiment_dir + "は作成済み")

  # 画像が格納されているフォルダ
  image_dir = "{}/{}".format(original_dir, feature_folder_name)

  # テストと訓練データ分割後の画像を格納するフォルダ
  base_dir = "{}/experiment/splited_test_{}_seed_{}".format(original_dir, irregular_label, str(seed))

  # テストと訓練データの分割
  image_dir_train_test_sprit(image_dir, base_dir, seed, train_size)

  original_dir = base_dir + "/train/{}/".format(irregular_label)
  output_dir = base_dir + "/validation/{}/".format(irregular_label)

  files_names = os.listdir(original_dir)
  for fname in files_names:
      src = os.path.join(original_dir, fname)
      dst = os.path.join(output_dir, fname)
      shutil.copyfile(src, dst)
      os.remove(src)
      print(src + "から" + dst + "をコピー完了")

  print("分割終了")



"""# EfficientNetによる異常検知"""

!pip install timm

"""ライブラリの読み込み"""

import os
import shutil
from PIL import Image, ImageDraw
import glob
import sys, os, urllib.request, tarfile, cv2
import numpy as np
import matplotlib.pyplot as plt
import torch.utils.data as data
from typing import Optional, List
import torch
import torch.nn.functional as F
from torchvision import models, transforms
import random
from tqdm.notebook import tqdm
from sklearn.covariance import LedoitWolf
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn import metrics
import timm
from timm.models.efficientnet import EfficientNet
from mpl_toolkits.mplot3d import Axes3D
import pandas as pd
from scipy.spatial import distance
import csv
import re
import time
import gc
import tracemalloc
import copy

"""画像解析の設定"""

# this cell need not modify if you want to use efficientnet as feature extractor
IMG_SIZE_MAP =  {
    'tf_efficientnet_b0_ns': 224,
    'tf_efficientnet_b1_ns': 240,
    'tf_efficientnet_b2_ns': 260,
    'tf_efficientnet_b3_ns': 300,
    'tf_efficientnet_b4_ns': 380,
    'tf_efficientnet_b5_ns': 456,
    'tf_efficientnet_b6_ns': 528,
    'tf_efficientnet_b7_ns': 600,
    'tf_efficientnet_l2_ns_475': 475,
    'tf_efficientnet_l2_ns': 800,
    # to check that the implementation is correct
    'tf_efficientnet_b4': 380,
    'tf_efficientnet_b7': 600,
}
LEVEL_COUNT = 9

####### input condition ############
save_RESIZE = 75 # imgサイズをマスク個数で割り切れる数値に
to_color = True
mask = False

level = 2
#model_name = 'tf_efficientnet_b6_ns'
#RESIZE = 528

model_name = 'tf_efficientnet_b7_ns'
RESIZE = 600

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device) #GPUが使用できているかの確認

model = timm.create_model(model_name, pretrained=True)
model.eval()
model.to(device)
################################

"""関数の定義

"""

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)  # type: ignore
    torch.backends.cudnn.deterministic = True  # type: ignore
    torch.backends.cudnn.benchmark = True  # type: ignore

def MVTechAD(download_dir, path):
    target_path = "data/"

    if not os.path.exists(download_dir):
        os.mkdir(download_dir)

    # download file
    def _progress(count, block_size, total_size):
        sys.stdout.write('\rDownloading %s %.2f%%' % (source_path,
            float(count * block_size) / float(total_size) * 100.0))
        sys.stdout.flush()

    source_path = path
    dest_path = os.path.join(download_dir, "data.tar.xz")
    urllib.request.urlretrieve(source_path, filename=dest_path, reporthook=_progress)
    # untar
    with tarfile.open(dest_path, "r:xz") as tar:
        tar.extractall(target_path)

class ImageTransform():
    def __init__(self, resize=RESIZE):
        self.data_transform = {
            'train': transforms.Compose([
                transforms.Resize(resize),  # リサイズ
                #transforms.RandomCrop(224),
                #transforms.RandomRotation(20, fill=200),
                #transforms.RandomHorizontalFlip(),  # データオーギュメンテーション
                transforms.ToTensor(),  # テンソルに変換
                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # 0-1 → 標準化
            ]),
            'val': transforms.Compose([
                transforms.Resize(resize),  # リサイズ
                #transforms.CenterCrop(224),  # 画像中央をresize×resizeで切り取り
                transforms.ToTensor(),  # テンソルに変換
                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # 0-1 → 標準化
            ])
        }

    def __call__(self, img, phase='train'):
        return self.data_transform[phase](img)


def fig_show(img):
    # 2. 元の画像の表示
    plt.imshow(img)
    plt.title("Original")
    plt.show()

    transform = ImageTransform()
    img_transformed = transform(img, phase="train")  # torch.Size([3, 224, 224])

    # (色、高さ、幅)を (高さ、幅、色)に変換し、0-1に値を制限して表示
    plt.subplot(1,2,1)
    img_transformed = img_transformed.numpy().transpose((1, 2, 0))
    img_transformed = np.clip(img_transformed, 0, 1)
    plt.imshow(img_transformed*255)
    plt.title("Train")
    plt.show()

set_seed(1213)

def extract_features(inputs: torch.Tensor,
                     model: EfficientNet,
                     level):
    features = dict()
    # extract stem features as level 1
    x = model.conv_stem(inputs)
    x = model.bn1(x)
    x = model.act1(x)
    features['level_1'] = F.adaptive_avg_pool2d(x, 1)
    # extract blocks features as level 2~8
    for i, block_layer in enumerate(model.blocks):
        x = block_layer(x)
        features[f'level_{i+2}'] = F.adaptive_avg_pool2d(x, 1)
    # extract top features as level
    x = model.conv_head(x)
    x = model.bn2(x)
    x = model.act2(x)
    features['level_9'] = F.adaptive_avg_pool2d(x, 1)
    return features['level_{}'.format(str(level))]

# 複数枚(Dataloader, Dataset格納)からの特徴抽出
def get_mean_cov(loader):
    feat = []

    for inputs in loader:
        inputs = inputs.to(device)
        # levelは1~9のint, featuresは上述のextract_features()結果
        feat_list = extract_features(inputs, model, level)
        feat_list = feat_list.cpu().detach().numpy()
        # print(feat_list.shape)
        for i in range(len(feat_list)):
            feat.append(feat_list[i].reshape(-1))

    feat = np.array(feat)

    mean = np.mean(feat, axis=0)
    cov = np.cov(feat.T)

    return feat, mean, cov

# --------異常スコアの算出---------------------
def get_score(feat, mean, cov):
    result = []
    # 分散共分散行列の逆行列を計算
    cov_i = np.linalg.pinv(cov)

    for i in range(len(feat)):
        result.append(distance.mahalanobis(feat[i], mean, cov_i))
    return result, cov_i

# --------混合行列の算出---------------------
def get_evaluation(normal_score, anomaly_score, y_true):
    # 混合行列を算出
    # y_pred: np.hstack((normal_score, anomaly_score))
    cm = confusion_matrix(y_true, np.hstack((normal_score, anomaly_score)))
    accuracy = accuracy_score(y_true, np.hstack((normal_score, anomaly_score)))

    return cm, accuracy

def get_auc(Z1, Z2, save_name):
    mahalanobis_path = '{}_mahalanobis.png'.format(save_name)
    fig = plt.figure()
    plt.title("Mahalanobis distance")
    plt.plot(Z1, label="normal")
    plt.plot(Z2, label="anomaly")
    plt.legend()
    plt.savefig(mahalanobis_path)
    plt.show()
    #plt.close(fig)
    #plt.savefig(mahalanobis_path)

    ## 正解ラベルの作成
    # 全て正常(0)として作成し、anormal_scoreに該当する部分を異常(1)に置換
    y_true = np.zeros(len(Z1)+len(Z2))
    y_true[len(Z1):] = 1#0:正常、1：異常

    # FPR, TPR(, しきい値) を算出
    #fpr, tpr, thres = metrics.roc_curve(y_true, y_pred) # hstack: 水平方向に連結
    fpr, tpr, _ = metrics.roc_curve(y_true, np.hstack((Z1, Z2)))

    # AUC
    auc = metrics.auc(fpr, tpr)

    return fpr, tpr, auc

def plot_roc(Z1, Z2, save_name, plot_name):
    fpr, tpr, auc1 = get_auc(Z1, Z2, save_name)
    roc_path = '{}_ROC.png'.format(save_name)
    
    fig = plt.figure()
    plt.plot(fpr, tpr, label=plot_name + '(AUC = %.3f)'%(auc1))
    plt.legend()
    plt.title(plot_name + '(ROC)')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.grid(True)
    plt.savefig(roc_path)
    plt.show()
    #plt.close(fig)
    #plt.savefig(roc_path)

    return auc1

"""マスクパターンを0/1の二値でランダム生成"""

'''マスクパターンの生成
Input:
    n_maskpatterns: マスクパターンの数 (e.g. 1000)
    n_masks: マスク候補の位置の数

Returns:
    mask_datasheet: 2d numpy array, num_maskpattens * num_masks
'''

if mask == True:
    # 縦軸と横軸で情報が異なる場合(Skeleton(松藤データ))
    n_mask_in_a_col =  75#@param {type:"integer"}
    n_mask_in_a_row_time =  15#@param {type:"integer"}
    n_masks = n_mask_in_a_row_time * n_mask_in_a_col#n_mask_in_a_col**2

    # MVtech: リサイズ後の全ての画素で実施
    #n_mask_in_a_col = RESIZE
    #n_masks = RESIZE ** 2

    masking_without_a_pixel = True #@param {type:"string"}
    identity_matrix = True #@param {type:"string"}


    if identity_matrix == True:
        # マスクをかける箇所は「0」
        # マスク以外の箇所を1つ指定
        if masking_without_a_pixel == True:
            mask_datasheet = np.eye(n_masks)
            mask_datasheet = np.insert(mask_datasheet, 0, 1, axis=0)
            n_maskpatterns = n_masks + 1

        # マスクの箇所を1つ指定
        else:
            mask_datasheet = np.eye(n_masks)
            mask_datasheet = abs(mask_datasheet - 1)
            mask_datasheet = np.insert(mask_datasheet, 0, 1, axis=0)
            n_maskpatterns = n_masks + 1
    else:
        np.random.seed(seed=32)
        mask_datasheet = np.random.randint(0, 2, (n_maskpatterns * n_masks)).reshape((n_maskpatterns, n_masks))
        mask_datasheet[0,:] = 1

    print("array_shape: ", mask_datasheet.shape)
    print("array: ", mask_datasheet)
else:
    n_maskpatterns = 1

if mask == True:
    ## マスクパターン(0, 1)の作成
    segments = []

    # マスクなしversionの作成(全てのpixelがオン / マスクがオフ:1が全て)
    #temp = np.ones((save_RESIZE, save_RESIZE, 3), dtype='int')
    temp = np.ones((save_RESIZE, save_RESIZE, 3), dtype='int')
    segments.append(temp)

    # マスクのサイズ
    feature_scale = int(save_RESIZE/n_mask_in_a_col) # 特徴量ごとに処理(列を指定)
    mask_scale = int(save_RESIZE/n_mask_in_a_row_time) # マスク/ピクセルの大きさ(行を指定)

    # マスクありversionを作成
    for col in range(n_mask_in_a_col):
        for row in range(n_mask_in_a_row_time):
            print("col: ",col)
            print("row: ",row)
            # マスクじゃないpixel位置(一箇所)とそれ以外はマスク箇所(1が一つ、0が他全て)
            if masking_without_a_pixel == True:
                # temp = np.zeros((save_RESIZE, save_RESIZE, 3), dtype='int')
                temp = np.zeros((save_RESIZE, save_RESIZE, 3), dtype='int')
                temp[row*mask_scale:(row+1)*mask_scale, col*feature_scale:(col+1)*feature_scale, :] = 1
                # numpy [行,列]
            else:
                # マスク位置(一箇所)とそれ以外はpixel(0が一つ、1が他全て)
                # temp = np.ones((save_RESIZE, save_RESIZE, 3), dtype='int')
                temp = np.ones((save_RESIZE, save_RESIZE, 3), dtype='int')
                temp[row*mask_scale:(row+1)*mask_scale, col*feature_scale:(col+1)*feature_scale, :] = 0
            temp = temp.astype(bool)
            segments.append(temp)

"""関数の定義[マスク画像の生成と読み込み]"""

def generate_maskImage(img_array, mask_datasheet, masking_idx):
    # 縦・横を指定した数値に分割し、マスク部分を０、それ以外を１とした行列作成
    # 元画像との乗算に使用
    ## 初回で計算済み
    #segments = ImageSegmentation(img_array)

    # 元画像のコピー(マスク処理用)
    temp = copy.deepcopy(img_array)
    
    # masking_idx番号に相当するマスクパターン
    #row = mask_datasheet[masking_idx]
    
    # マスクをかける箇所(0)のマスク番号（列番号）を抽出
    #zeros = np.where(row == 0)[0]
    
    # マスクをかける箇所(0)を元画像のコピー(temp)に乗算
    # 0 * 画素値の計算で、画素値を削除（マスク）する。
    #for z in zeros:
    #    temp = segments[z].astype(int) * temp

    ## マスク番号にあったマスクパターンのみを乗算
    temp = segments[masking_idx].astype(int) * temp
    
    return temp

# マスクを施すデータセット作成関数
class MaskDataset(data.Dataset):
    def __init__(self,
                 train_list: List[List[str]],
                 to_color="False",
                 masking_idx=0):
                 
        self.file_list = train_list
        self.to_color = to_color
        self.transform = ImageTransform()
        self.masking_idx = masking_idx

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx: int):
        # 訓練、テスト画像のパスを格納したリスト
        img_path = self.file_list[idx]
        if self.to_color:
            img = Image.open(img_path).convert("RGB")  # [高さ][幅][色RGB]
        else:
            img = Image.open(img_path)

        if mask == True:
            img= img.resize((save_RESIZE, save_RESIZE))
            # PIL Image -> np.array
            img_array = np.array(img)
            
            # getitem関数による画像idx・masking_idxに対応するマスク画像arrayを算出
            masked_img_array = generate_maskImage(img_array, mask_datasheet, self.masking_idx)
            
            # np.array -> PIL Image
            img = Image.fromarray(masked_img_array.astype(np.uint8))

        # Imageをリサイズ / ToTensor / RGBの正規化
        img = self.transform(img, "train")
        
        return img

"""データセットの定義"""

#@title
'''
フォルダの設定
    メインのフォルダ名 -> ユーザの名のフォルダ
                                -> 学習と検証データを含めたフォルダ名
                                -> train / validationの、2フォルダで以下共通 
                                -> label_0 / label_1の、2フォルダで以下共通
                                -> それぞれの画像ファイル(png形式)

    ここのブロックで、上記のファイル名をそれぞれ入力
'''
hazumi_version = "1712" #@param {type:"string"}
main_folder_name = "Hazumi/Hazumi{}/png".format(hazumi_version)
main_folder_dir = "/content/drive/My Drive/Colab Notebooks/{}".format(main_folder_name)

# ラベルごとの画像を格納したフォルダをregular, irregularとして指定
# regularとして扱う画像を格納したフォルダ名
regular_label = "label_1" #@param {type:"string"}

# irregularとして扱う画像を格納したフォルダ名
irregular_label = "label_0" #@param {type:"string"}

# 使用する特徴量のフォルダを指定
feature_folder_name = "frame_sub" #@param {type:"string"}
seed_idx = 1 #@param {type:"integer"}

train_test_split_folder_name = "splited_test_{}_seed_{}".format(irregular_label, str(seed_idx))
train_path = train_test_split_folder_name + "/train"
test_path = train_test_split_folder_name + "/validation"

# 結果を記録するフォルダの作成
result_folder_name = "test" #@param {type:"string"}
result_path = "{}/result/{}".format(main_folder_dir, result_folder_name)
try:
    result_folder = "{}/result".format(main_folder_dir)
    os.mkdir(result_folder)
except FileExistsError:
    print(result_folder + "は作成済み")
try:
    os.mkdir(result_path)
except FileExistsError:
    print(result_path + "は作成済み")
try:
    result_dir = os.path.join(result_path, train_test_split_folder_name)
    os.mkdir(result_dir)
except FileExistsError:
    print(result_dir + "は作成済み")

if hazumi_version == "1911":
  # Hazumi1911
  user_list = ["1911F2001", "1911F2002", "1911F3001", "1911F3002", "1911F3003",
              "1911F4001", "1911F4002", "1911F4003", "1911F5001", "1911F5002",
              "1911F6001", "1911F6002", "1911F6003", "1911F7002", "1911M2001", 
              "1911M2002", "1911M2003", "1911M4001", "1911M4002", "1911M5001",
              "1911M5002", "1911M6001", "1911M6002", "1911M6003", "1911M7001", "1911M7002"]
elif hazumi_version == "1902":
  # Hazumi1902
  # dumpfileに1902F4007.csv,1902M4003.csvが含まれていなかった
  user_list = ["1902F2001", "1902F2002", "1902F3001", "1902F3002", "1902F4001",
              "1902F4002", "1902F4003", "1902F4004", "1902F4005", "1902F4006",
              "1902F4008", "1902F4009", "1902F4010", "1902F4011", "1902F6001", 
              "1902F6002", "1902F7001", "1902F7002", "1902F7003", "1902M2001",
              "1902M3001", "1902M4001", "1902M4002", "1902M5001", "1902M5002", 
              "1902M5003", "1902M7001", "1902M7002"]
  # 1902F7001を除外
  user_list = ["1902F2001", "1902F2002", "1902F3001", "1902F3002", "1902F4001",
              "1902F4002", "1902F4003", "1902F4004", "1902F4005", "1902F4006",
              "1902F4008", "1902F4009", "1902F4010", "1902F4011", "1902F6001", 
              "1902F6002", "1902F7002", "1902F7003", "1902M2001", "1902M3001", 
               "1902M4001", "1902M4002", "1902M5001", "1902M5002", "1902M5003",
               "1902M7001", "1902M7002"]
elif hazumi_version == "1902":
  # Hazumi1712
  # dumpfileに1712F4002.csv,1712M5003.csvが含まれていなかった
  user_list = ["1712F2006", "1712F2010", "1712F2018", "1712F2019", "1712F3013",
              "1712F3016", "1712F3030", "1712F4011", "1712F4012", "1712F4017",
              "1712F4027", "1712F5008", "1712F5014", "1712F5026", "1712M2007", 
              "1712M2021", "1712M2024", "1712M2028", "1712M2029", "1712M3001",
              "1712M3005", "1712M3009", "1712M4004", "1712M4015", "1712M4020", 
              "1712M4022", "1712M4025"]
print(user_list)

'''
EfficientNet
'''

auc_list = []
for file_idx in range(0, len(user_list)):
    '''
    該当ユーザの訓練、テスト画像のパスを取得
    '''
    start_user = time.time()
    target_folder_name = main_folder_dir + "/" + user_list[file_idx] + "/experiment"
    print(target_folder_name)
    
    # Regularと指定したラベルに従って、学習データと検証データの画像が格納されたフォルダへのパスを指定
    train_img_file_path = "{}/{}/{}".format(target_folder_name, train_path, regular_label)
    test_img_file_path = "{}/{}".format(target_folder_name, test_path)
    train_list = glob.glob(train_img_file_path + "/**.png")
    test_list = glob.glob(test_img_file_path + "/**/**.png")

    normal_list = glob.glob("{}/{}/**.png".format(test_img_file_path, regular_label))
    anomaly_list = glob.glob("{}/{}/**.png".format(test_img_file_path, irregular_label))

    print("学習データ(正常データのみ) : ",len(train_list))
    print("テストデータ(正常、異常のmix) : ",len(test_list))
    print("テストデータ(正常) : ",len(normal_list))
    print("テストデータ(異常) : ",len(anomaly_list))

    # DataFrame形式で画像ファイル名と異常スコアを保存
    normal_df = pd.DataFrame({"normal_file_list": normal_list})
    anomaly_df = pd.DataFrame({"anomaly_file_list": anomaly_list})

    '''
    EfficientNetを用いた全マスク画像の特徴抽出
    正解分布作成と異常データのマハラノビス距離の計算
    '''
    for masking_idx in range(0, 1):#n_maskpatterns):
        start_mask = time.time()
        
        # マスクを施したデータセットを作成
        train_dataset = MaskDataset(
                                    train_list,
                                    to_color=to_color,
                                    masking_idx=masking_idx
                                    )
        normal_dataset = MaskDataset(
                                    normal_list,
                                    to_color=to_color,
                                    masking_idx=masking_idx
                                    )
        anomaly_dataset = MaskDataset(
                                    anomaly_list,
                                    to_color=to_color,
                                    masking_idx=masking_idx
                                    )

        # 各データセットをPytorchの学習に用いるデータ形式へ変更
        # 今回はバッチサイズは1にしているため、データ形式を変換しているのみ
        train_loader = data.DataLoader(
                                        train_dataset,
                                        batch_size=1,
                                        shuffle=False,
                                        num_workers=2,
                                        pin_memory=True,
                                        drop_last=True
                                        )

        normal_loader = data.DataLoader(
                                        normal_dataset,
                                        batch_size=1,
                                        shuffle=False,
                                        num_workers=2,
                                        # pin_memory=True,
                                        drop_last=True
                                        )

        anomaly_loader = data.DataLoader(
                                        anomaly_dataset,
                                        batch_size=1,
                                        shuffle=False,
                                        num_workers=2,
                                        # pin_memory=True,
                                        drop_last=True
                                        )

        # 複数画像のEfficientNetの中間層での特徴を抽出
        # 訓練画像での特徴量の平均、分散を計算
        train_feat, mean, cov = get_mean_cov(train_loader)
        normal_feat, _, _ = get_mean_cov(normal_loader)
        anomaly_feat, _, _ = get_mean_cov(anomaly_loader)

        # 「訓練画像の平均と分散」とテストデータを比較し、異常スコアを算出
        normal_score, cov_i = get_score(normal_feat, mean, cov)
        anomaly_score, _ = get_score(anomaly_feat, mean, cov)
        # Numpyへ変換
        normal_score = np.array(normal_score)
        anomaly_score = np.array(anomaly_score)

        # 各異常スコアを各マスキング状況でDataFrame形式で保存
        normal_score_df = pd.DataFrame({masking_idx: normal_score})
        anomaly_score_df = pd.DataFrame({masking_idx: anomaly_score})
        
        # オリジナル画像での異常スコアを用いてROC曲線を描画
        ## google driveに保存
        save_path = "{}/{}/{}".format(result_path, train_test_split_folder_name, user_list[file_idx])
        plot_path = user_list[file_idx]
        auc = plot_roc(normal_score, anomaly_score, save_path, plot_path)#train(フォルダ名)はグラフ化の際に使用
        auc_list.append(auc)

        # DataFrameの更新
        normal_df = pd.concat([normal_df, normal_score_df], axis=1)
        anomaly_df = pd.concat([anomaly_df, anomaly_score_df], axis=1)
        print("user: ", user_list[file_idx], ", mask_id: ", masking_idx)

        # ユーザごとの全てのプロセス終了までに経過した時間
        elapsed_time_mask = time.time() - start_mask
        print ("mask_process_elapsed_time:{0}".format(elapsed_time_mask) + "[sec]")

    # DataFrame -> csv
    csv_name_normal = "{}/{}/{}_{}_normal_df.csv".format(result_path, train_test_split_folder_name, user_list[file_idx], regular_label)
    normal_df.to_csv(csv_name_normal)
    
    csv_name_anomaly = "{}/{}/{}_{}_anomaly_df.csv".format(result_path, train_test_split_folder_name, user_list[file_idx], regular_label)
    anomaly_df.to_csv(csv_name_anomaly)

    # ユーザごとの終了までに経過した時間
    elapsed_time_user = time.time() - start_user
    print ("user_process_elapsed_time:{0}".format(elapsed_time_user) + "[sec]")

auc_df = pd.DataFrame(auc_list)
auc_df.index = user_list
csv_save_path = "{}/{}/auc.csv".format(result_path, train_test_split_folder_name)
auc_df.to_csv(csv_save_path)

print(auc_df)

auc_df.head(-1)

auc_df.tail()

"""# 異常検知(マスク処理含む)の結果解析

マスク処理実施後の結果を活用。GPUを長時間使用するので、高間研究室との合同PCで「EfficientNetによる異常検知」を実行したほうが良い

(Google Colabは長時間のGPU使用はできないため)
"""

!pip install tslearn

import os
import shutil
from PIL import Image, ImageDraw
import glob
import sys, os, urllib.request, tarfile, cv2
import numpy as np
import matplotlib.pyplot as plt
import torch.utils.data as data
from typing import Optional, List
import torch
import torch.nn.functional as F
from torchvision import models, transforms
import random
from tqdm.notebook import tqdm
from sklearn.covariance import LedoitWolf
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn import metrics
from mpl_toolkits.mplot3d import Axes3D
import pandas as pd
from scipy.spatial import distance
import csv
import re
import random
import time
import gc
import tracemalloc
import copy

def minmax255_norm(df_input):
    return df_input.apply(lambda x: (x-x.min())/(255 - x.min()), axis=0)

# 標準化
def std_norm(df_input):
    return df_input.apply(lambda x: (x-x.mean())/ x.std(), axis=1)  # 行方向 axis=1

def std_norm_axis0(df_input):
    return df_input.apply(lambda x: (x-x.mean())/ x.std(), axis=0)  # 行方向 axis=1

# 正規化（最も元画像よりも異常値が減少したものが最大値）
def minmax_norm(df_input):
    return df_input.apply(lambda x: (x-x.min())/(x.max() - x.min()), axis=1)

def minmax_norm_axis0(df_input):
    return df_input.apply(lambda x: (x-x.min())/(x.max() - x.min()), axis=0)

def min_max(x, axis=None):
    min = x.min(axis=axis, keepdims=True)
    max = x.max(axis=axis, keepdims=True)
    result = (x-min)/(max-min)
    return result
    
# 正規化(正常との距離を最大値)
def distance_from_normal_norm(df_input, csv_df_original):
    return csv_df.div(csv_df_original['Original'], axis=0)# 最小値は０なので、正規化の式から最小値を削除


def dist(x, y):
    return (x - y)**2

def get_min(m0, m1, m2, i, j):
    if m0 < m1:
        if m0 < m2:
            return i - 1, j, m0
        else:
            return i - 1, j - 1, m2
    else:
        if m1 < m2:
            return i, j - 1, m1
        else:
            return i - 1, j - 1, m2

def partial_dtw(x, y):
    Tx = len(x)
    Ty = len(y)

    C = np.zeros((Tx, Ty))
    B = np.zeros((Tx, Ty, 2), int)

    C[0, 0] = dist(x[0], y[0])
    for i in range(Tx):
        C[i, 0] = dist(x[i], y[0])
        B[i, 0] = [0, 0]

    for j in range(1, Ty):
        C[0, j] = C[0, j - 1] + dist(x[0], y[j])
        B[0, j] = [0, j - 1]

    for i in range(1, Tx):
        for j in range(1, Ty):
            pi, pj, m = get_min(C[i - 1, j],
                                C[i, j - 1],
                                C[i - 1, j - 1],
                                i, j)
            C[i, j] = dist(x[i], y[j]) + m
            B[i, j] = [pi, pj]
    t_end = np.argmin(C[:,-1])
    cost = C[t_end, -1]
    
    path = [[t_end, Ty - 1]]
    i = t_end
    j = Ty - 1

    while (B[i, j][0] != 0 or B[i, j][1] != 0):
        path.append(B[i, j])
        i, j = B[i, j].astype(int)
        
    return np.array(path), cost


def dtw(x, y):
    Tx = len(x)
    Ty = len(y)

    C = np.zeros((Tx, Ty))
    B = np.zeros((Tx, Ty, 2), int)

    C[0, 0] = dist(x[0], y[0])
    for i in range(Tx):
        C[i, 0] = C[i - 1, 0] + dist(x[i], y[0])
        B[i, 0] = [i-1, 0]

    for j in range(1, Ty):
        C[0, j] = C[0, j - 1] + dist(x[0], y[j])
        B[0, j] = [0, j - 1]

    for i in range(1, Tx):
        for j in range(1, Ty):
            pi, pj, m = get_min(C[i - 1, j],
                                C[i, j - 1],
                                C[i - 1, j - 1],
                                i, j)
            C[i, j] = dist(x[i], y[j]) + m
            B[i, j] = [pi, pj]
    cost = C[-1, -1]
    
    path = [[Tx - 1, Ty - 1]]
    i = Tx - 1
    j = Ty - 1

    while ((B[i, j][0] != 0) or (B[i, j][1] != 0)):
        path.append(B[i, j])
        i, j = B[i, j].astype(int)
        #cost = cost + C[i,j]
        #print(cost)
    path.append([0,0])
    return np.array(path), cost, C

# 複数の部分一致箇所の発見
def spring(x, y, epsilon):
    Tx = len(x)
    Ty = len(y)

    C = np.zeros((Tx, Ty))
    B = np.zeros((Tx, Ty, 2), int)
    S = np.zeros((Tx, Ty), int)

    C[0, 0] = dist(x[0], y[0])

    for j in range(1, Ty):
        C[0, j] = C[0, j - 1] + dist(x[0], y[j])
        B[0, j] = [0, j - 1]
        S[0, j] = S[0, j - 1]
        
    for i in range(1, Tx):
        C[i, 0] = dist(x[i], y[0])
        B[i, 0] = [0, 0]
        S[i, 0] = i
        
        for j in range(1, Ty):
            pi, pj, m = get_min(C[i - 1, j],
                                C[i, j - 1],
                                C[i - 1, j - 1],
                                i, j)
            C[i, j] = dist(x[i], y[j]) + m
            B[i, j] = [pi, pj]
            S[i, j] = S[pi, pj]
            
        imin = np.argmin(C[:(i+1), -1])
        dmin = C[imin, -1]
        
        if dmin > epsilon:
            continue
            
        for j in range(1, Ty):
            if (C[i,j] < dmin) and (S[i, j] < imin):
                break
        else:
            path = [[imin, Ty - 1]]
            temp_i = imin
            temp_j = Ty - 1
            
            while (B[temp_i, temp_j][0] != 0 or B[temp_i, temp_j][1] != 0):
                path.append(B[temp_i, temp_j])
                temp_i, temp_j = B[temp_i, temp_j].astype(int)
                
            C[S <= imin] = 100000000
            yield np.array(path), dmin

def plot_path(paths, A, B, D):
    plt.figure(figsize=(5,5))
    gs = gridspec.GridSpec(2, 2,
                       width_ratios=[1,5],
                       height_ratios=[5,1]
                       )
    ax1 = plt.subplot(gs[0])
    ax2 = plt.subplot(gs[1])
    ax4 = plt.subplot(gs[3])

    ax2.pcolor(D, cmap=plt.cm.Blues)
    ax2.get_xaxis().set_ticks([])
    ax2.get_yaxis().set_ticks([])
    
    for path in paths:
        ax2.plot(path[:,0]+0.5, path[:,1]+0.5, c="C3")
    
    ax4.plot(A)
    ax4.set_xlabel("$X$")
    ax1.invert_xaxis()
    ax1.plot(B, range(len(B)), c="C1")
    ax1.set_ylabel("$Y$")

    ax2.set_xlim(0, len(A))
    ax2.set_ylim(0, len(B))
    plt.show()

# dumpfileに1911F7001.csvなどが含まれていなかったので、リストから削除
#user_list = ["1911F2001", "1911F2002", "1911F3001", "1911F3002", "1911F3003",
#             "1911F4001", "1911F4002", "1911F4003", "1911F5001", "1911F5002",
#             "1911F6001", "1911F6002", "1911F6003", "1911F7002", "1911M2001", 
#             "1911M2002", "1911M2003", "1911M4001", "1911M4002", "1911M5001",
#             "1911M5002", "1911M6001", "1911M6002", "1911M6003", "1911M7001", "1911M7002"]

#user_list = ["1911F2001", "1911F2002", "1911F3001", "1911F3002", "1911F3003"]
user_list = ["1712F2006", "1712F2010", "1712F2018", "1712F2019", "1712F3013",
              "1712F3016", "1712F3030", "1712F4011", "1712F4012", "1712F4017",
              "1712F4027", "1712F5008", "1712F5014", "1712F5026", "1712M2007", 
              "1712M2021", "1712M2024", "1712M2028", "1712M2029", "1712M3001",
              "1712M3005", "1712M3009", "1712M4004", "1712M4015", "1712M4020", 
              "1712M4022", "1712M4025"]

'''
フォルダの設定
    メインのフォルダ名 -> ユーザの名のフォルダ
                                -> 学習と検証データを含めたフォルダ名
                                -> train / validationの、2フォルダで以下共通 
                                -> label_0 / label_1の、2フォルダで以下共通
                                -> それぞれの画像ファイル(png形式)

    ここのブロックで、上記のファイル名をそれぞれ入力
'''
hazumi_version = "1712" #@param {type:"string"}
main_folder_name = "Hazumi/Hazumi{}/png".format(hazumi_version)
main_folder_dir = "/content/drive/My Drive/Colab Notebooks/{}".format(main_folder_name)

# ラベルごとの画像を格納したフォルダをregular, irregularとして指定
# regularとして扱う画像を格納したフォルダ名
regular_label = "label_1" #@param {type:"string"}

# irregularとして扱う画像を格納したフォルダ名
irregular_label = "label_0" #@param {type:"string"}

# 使用する特徴量のフォルダを指定
feature_folder_name = "frame_sub" #@param {type:"string"}
seed_idx = 1 #@param {type:"integer"}

train_test_split_folder_name = "splited_test_{}_seed_{}".format(irregular_label, str(seed_idx))
train_path = train_test_split_folder_name + "/train"
test_path = train_test_split_folder_name + "/validation"

# 異常検知結果が記載されたcsvデータが格納されたフォルダの指定
result_folder_name = "test" #@param {type:"string"}
result_path = "{}/result/{}".format(main_folder_dir, result_folder_name)

'''
    入力：ユーザ番号
    出力：全発話の異常スコアの平均をDataFrameに表現
'''
def anomaly_score_to_dataframe(user_id):
    # 異常検知結果を読み込むユーザ
    user_name = user_list[user_id]
    print("user_name: ", user_name)

    # 異常検知結果を記載したcsvデータの読み込み
    anomaly_result_path = "{}/{}/{}_{}_anomaly_df.csv".format(result_path, train_test_split_folder_name, user_name, regular_label)
    anomaly_scores_df = pd.read_csv(anomaly_result_path)
    print("画像の枚数: ", len(anomaly_scores_df))

    # マスク処理を行わなくても、正常と区別ができない異常画像(=異常スコアが著しく低いもの)は除外
    outlier_exclude_maharanobis =  0#@param {type:"integer"}
    if outlier_exclude_maharanobis > 0:
        anomaly_scores_df = anomaly_scores_df[anomaly_scores_df["0"] >= outlier_exclude_maharanobis]

    # マスク処理なし、及びマスク処理で得られた異常スコアを抽出
    anomaly_scores_df = anomaly_scores_df.loc[:, "0":]

    # 各画像の全てのマスクパターンの異常スコアで正規化(0~1)
    anomaly_scores_norm = minmax_norm(anomaly_scores_df)

    # マスク処理を行わないときの異常スコアとの差分を計算
    anomaly_scores_norm = anomaly_scores_norm.sub(anomaly_scores_norm["0"], axis=0)
    
    # 全ての画像の異常スコアを平均
    anomaly_scores_norm_mean = anomaly_scores_norm.mean(axis = 0)
    anomaly_scores_norm_mean.name = user_name

    ## 貢献度の大きい数値出力用
    anomaly_scores_norm_mean_sort = anomaly_scores_norm_mean.sort_values(ascending=False)
    print("絶対値合計-変化量の大きい箇所")
    print(user_name)
    print(anomaly_scores_norm_mean_sort.index[0]) # 一番大きい値
    print(anomaly_scores_norm_mean_sort.index[1]) # 二番目に大きい値
    print(anomaly_scores_norm_mean_sort.index[2]) # 三番目に大きい値
    print(anomaly_scores_norm_mean_sort.index[3]) # 4番目に大きい値
    print(anomaly_scores_norm_mean_sort.index[4]) # 5番目に大きい値
    #####################

    return anomaly_scores_norm_mean

'''
異常検知結果を記録したcsvを読み込み

列：テスト画像名、通常画像での異常スコア、マスクパターン１の異常スコア、マスクパターン２の異常スコア、・・・・
行：全てのテスト画像（-> 画像は発話区間の時系列情報）
'''
# 全員分の異常検知の結果を読み込み
for user_id in range(0, len(user_list)):
    anomaly_scores_norm_mean = anomaly_score_to_dataframe(user_id)

    ## 全てのユーザを横に繫げていく(縦：各マスク、横：ユーザ)
    if user_id == 0:
        anomaly_scores_norm_mean_all_user = anomaly_scores_norm_mean
    else:
        anomaly_scores_norm_mean_all_user = pd.concat([anomaly_scores_norm_mean_all_user, anomaly_scores_norm_mean], axis = 1)
    ######################

plt.figure()
anomaly_scores_norm_mean.plot(legend = True)
plt.xlabel("mask patterns")
plt.ylabel("maharanobis distance")

# 各ユーザの全てのテスト画像の平均に対して正規化 (DTW用)
# オリジナル異常画像=0を削除して正規化
anomaly_scores_norm_mean_all_user_norm = anomaly_scores_norm_mean_all_user.drop("0", axis = 0)
anomaly_scores_norm_mean_all_user_norm = minmax_norm_axis0(anomaly_scores_norm_mean_all_user_norm)

plt.figure()
anomaly_scores_norm_mean_all_user_norm.plot(legend = True)
plt.xlabel("mask patterns")
plt.ylabel("maharanobis distance")

"""## DTWによる時系列情報の類似度計算 """

import numpy as np
import pandas as pd
from tslearn.preprocessing import TimeSeriesScalerMeanVariance
from tslearn import metrics
from tslearn.utils import to_time_series_dataset
from pandas import DataFrame, Series

df_dtw = pd.DataFrame(index = user_list, columns= user_list)
for user_id in range(len(user_list)):
    for user_0_id in range(len(user_list)):
        user_name1 = user_list[user_0_id]
        user_name2 = user_list[user_id]

        path, sim = metrics.dtw_path(anomaly_scores_norm_mean_all_user_norm.iloc[:, user_0_id], anomaly_scores_norm_mean_all_user_norm.iloc[:, user_id])
        #print("{}, {}: {}".format(user_name1, user_name2, sim))

        df_dtw.iloc[user_0_id, user_id] = sim
df_dtw.head(-1)
#df_dtw.to_csv("折れ線グラフ全体_全ユーザ.csv")

'''
DTW結果を実際の曲線から確認する用
使い方：dataframe.loc[:, (ユーザ名1，ユーザ名2)].plot(legent=True)
'''
plt.figure()
anomaly_scores_norm_mean_all_user_norm.loc[:, ("1911F2001","1911F2002")].plot(legend = True) # DTW距離：3.55
anomaly_scores_norm_mean_all_user_norm.loc[:, ("1911F2001","1911F3001")].plot(legend = True) # DTW距離：2.04
plt.xlabel("mask patterns")
plt.ylabel("maharanobis distance")

"""## 異常スコアのヒートマップの作成"""

'''
    入力：ユーザ番号
    出力：全発話の異常スコアの平均を描画
'''
def average_anomaly_score(user_id):
    user_name = user_list[user_id]
    print("user_name: ", user_name)

    # 異常検知結果を記載したcsvデータの読み込み
    anomaly_result_path = "{}/{}/{}_{}_anomaly_df.csv".format(result_path, train_test_split_folder_name, user_name, regular_label)
    anomaly_scores_df = pd.read_csv(anomaly_result_path)
    print("画像の枚数: ", len(anomaly_scores_df))


    # テスト画像の枚数分で異常スコアの平均を算出
    ## 全ての画像で共通しそうな異常領域の推定が目的
    for img_id in range(0, len(anomaly_scores_df)):
        anomaly_a_score_df = anomaly_scores_df.iloc[0, 3:] # 数字、ファイル名、マスクなしを削除(2列目〜)

        # DataFrame -> Numpy
        anomaly_a_score_array = anomaly_a_score_df.values

        # 正規化
        anomaly_a_score_array  = min_max(anomaly_a_score_array ) * 255

        # 二次元に並び換え(特徴量の数、時間の分割数)
        anomaly_a_score_array  = anomaly_a_score_array.reshape(75, -1).T

        if img_id == 0:
            anomaly_scores_array_all = anomaly_a_score_array
        else:
            anomaly_scores_array_all = anomaly_scores_array_all + anomaly_a_score_array

    anomaly_scores_array_all = anomaly_scores_array_all / len(anomaly_scores_df)
    # ヒートマップを画像化表示
    plt.imshow(np.uint8(anomaly_scores_array_all))
    return anomaly_scores_array_all

df = average_anomaly_score(0)

df = average_anomaly_score(1)



"""## マスク区間とそれ以外の領域での統計量の比較"""

def calculate_statistics_values_at_mask_and_others(user_id, anomaly_threshold):
    user_name = user_list[user_id]  # ユーザの名前
    print(user_name)

    '''
    時系列情報を記録した画像の読み込み設定
    '''
    # 訓練・テスト画像が格納されたフォルダのパスを指定
    train_img_dir = "{}/{}/experiment/{}/train".format(main_folder_dir, user_name, train_test_split_folder_name)
    validation_img_dir = "{}/{}/experiment/{}/validation".format(main_folder_dir, user_name, train_test_split_folder_name)
    train_img_label1_path = '{}/label_1'.format(train_img_dir)
    validation_img_label1_path = '{}/label_1'.format(validation_img_dir)
    validation_img_label0_path = '{}/label_0'.format(validation_img_dir)

    # 訓練・テスト画像が格納されたフォルダから「.png」画像ファイルを探索
    train_img_label1_list = glob.glob(train_img_label1_path + "/**.png")
    validation_img_label1_list = glob.glob(validation_img_label1_path + "/**.png")
    validation_img_label0_list = glob.glob(validation_img_label0_path + "/**.png")

    # 異常スコアのヒートマップを計算
    anomaly_scores_norm_mean = anomaly_score_to_dataframe(user_id)

    # 異常スコアのヒートマップで、一定以上の異常スコアの箇所のみを「異常領域」として扱う
    over_threshold = anomaly_scores_norm_mean > anomaly_threshold
    num_masks = over_threshold.sum()
    print(num_masks)

    '''
    正常画像の平均値を算出
    -正常画像と異常画像の差分を扱う場合のみ使用-
    '''
    # 訓練画像のラベル1（正常）の画素値の平均を算出
    for train_img_label1_id in range(0, len(train_img_label1_list)):
        # 画像読み込み
        pil_img = Image.open(train_img_label1_list[train_img_label1_id])

        # PIL image -> Numpy
        img_np = np.array(pil_img)

        # 各画像の画素値を足し合わせる
        if train_img_label1_id == 0:
            train_img_label1_ave = img_np
        else:
            train_img_label1_ave = train_img_label1_ave + img_np
    # 平均を計算
    train_img_label1_ave = train_img_label1_ave/len(train_img_label1_list)


    '''
    異常画像中の「異常」と推定された領域とそれ以外の領域の統計量を算出
    '''
    for img_id in range(0, len(validation_img_label0_path)):
        # 画像読み込み
        img_pil = Image.open(validation_img_label0_path[img_id])
        print("テスト画像数: ", len(validation_img_label0_path))

        # PIL image -> Numpy
        img_array = np.array(img_pil)

        # 異常画像1枚 - 正常画像(訓練)の平均
        # img_array = img_array - train_img_label1_ave

        # Numpy -> DataFrame
        img_df = pd.DataFrame(img_array)


        '''
        マスク処理を加えた画像（時系列情報）
        マスク領域だけの画像（時系列情報）を用意
        '''
        # マスクの数を指定
        mask_areas = np.ones(1125)
        not_mask_areas = np.zeros(1125)

        # 異常スコアが高い順に並び換え
        anomaly_scores_norm_mean_sort = anomaly_scores_norm_mean.sort_values(ascending=False)

        # 異常スコアが閾値を超えたマスクの数だけ、領域を指定
        for mask_id in range(0, num_masks):
            mask_areas[int(anomaly_scores_norm_mean_sort.index[mask_id]) - 1] = 0
            not_mask_areas[int(anomaly_scores_norm_mean_sort.index[mask_id]) - 1] = 1

        # マスク後の画像を生成
        mask_areas  = mask_areas.reshape(75, -1).T
        masked_img_array = img_array * mask_areas
        masked_img_df = pd.DataFrame(masked_img_array)

        # マスク領域だけ残した画像を生成
        not_mask_areas  = not_mask_areas.reshape(75, -1).T
        only_mask_area_array = img_array * not_mask_areas
        only_mask_area_df = pd.DataFrame(only_mask_area_array)

        # 保存したい統計量
        average_list = []
        max_list = []
        min_list = []
        std_list = []
        var_list = []

        # 各特徴量の時系列情報にて統計量を算出
        for col_id in range(0, 75):
            print("元画像", img_df[col_id].mean())
            # マスク領域を削除した他の領域を抽出(0 = マスクを削除)
            drop_index = masked_img_df.index[masked_img_df[col_id] == 0]
            dropped_masked_img_df = masked_img_df.drop(drop_index)
            print("マスク以外の領域の平均: ", dropped_masked_img_df[col_id].mean())

            # マスク領域のみを抽出(0の部分を削除)
            drop_index = only_mask_area_df.index[only_mask_area_df[col_id] == 0]
            dropped_only_mask_area_df = only_mask_area_df.drop(drop_index)
            print("マスク領域のみの平均: ", dropped_only_mask_area_df[col_id].mean())

            # リストに各状況での平均値を格納
            average_list.append(img_df[col_id].mean())# 元画像
            average_list.append(dropped_masked_img_df[col_id].mean()) # マスク領域が付与された画像
            average_list.append(dropped_only_mask_area_df[col_id].mean()) # マスク領域のみの画像

            max_list.append(img_df[col_id].max())# 元画像
            max_list.append(dropped_masked_img_df[col_id].max()) # マスク領域が付与された画像
            max_list.append(dropped_only_mask_area_df[col_id].max()) # マスク領域のみの画像

            min_list.append(img_df[col_id].min())# 元画像
            min_list.append(dropped_masked_img_df[col_id].min()) # マスク領域が付与された画像
            min_list.append(dropped_only_mask_area_df[col_id].min()) # マスク領域のみの画像

            std_list.append(img_df[col_id].std())# 元画像
            std_list.append(dropped_masked_img_df[col_id].std()) # マスク領域が付与された画像
            std_list.append(dropped_only_mask_area_df[col_id].std()) # マスク領域のみの画像

            var_list.append(img_df[col_id].var())# 元画像
            var_list.append(dropped_masked_img_df[col_id].var()) # マスク領域が付与された画像
            var_list.append(dropped_only_mask_area_df[col_id].var()) # マスク領域のみの画像

        # リスト -> DataFrame
        average_df = pd.DataFrame(average_list, index = index_list, columns = [str(img_id)])#columns = [user_list[user_idx]])
        max_df = pd.DataFrame(max_list, index = index_list, columns = [str(img_id)])#columns = [user_list[user_idx]])
        min_df = pd.DataFrame(min_list, index = index_list, columns = [str(img_id)])#columns = [user_list[user_idx]])
        std_df = pd.DataFrame(std_list, index = index_list, columns = [str(img_id)])#columns = [user_list[user_idx]])
        var_df = pd.DataFrame(var_list, index = index_list, columns = [str(img_id)])#columns = [user_list[user_idx]])

        if img_id == 0:
            average_df_all_img = average_df
            max_df_all_img = max_df
            min_df_all_img = min_df
            std_df_all_img = std_df
            var_df_all_img = var_df
        else:
            average_df_all_img = pd.concat([average_df_all_img, average_df], axis = 1)
            max_df_all_img = pd.concat([max_df_all_img, max_df], axis = 1)
            min_df_all_img = pd.concat([min_df_all_img, max_df], axis = 1)
            std_df_all_img = pd.concat([std_df_all_img, std_df], axis = 1)
            var_df_all_img = pd.concat([var_df_all_img, std_df], axis = 1)

    return average_df_all_img, max_df_all_img, min_df_all_img, std_df_all_img, var_df_all_img

# 大小関係をカウント
def count_compaired(df):
    x_count = df.loc["masked_x", :] < df.loc["x", :]
    y_count = df.loc["masked_y", :] < df.loc["y", :]
    z_count = df.loc["masked_z", :] < df.loc["z", :]
    pitch_count = df.loc["masked_pitch", :] < df.loc["pitch", :]
    yaw_count = df.loc["masked_yaw", :] < df.loc["yaw", :]
    roll_count = df.loc["masked_roll", :] < df.loc["roll", :]
    print("-----マスク領域のほうが大きい画像数-----")
    print("全画像数: ", len(df.columns))
    print("x: ", x_count.sum())
    print("y: ", y_count.sum())
    print("z: ", z_count.sum())
    print("pitch: ", pitch_count.sum())
    print("yaw: ", yaw_count.sum())
    print("roll: ", roll_count.sum())
    return x_count, y_count, z_count, pitch_count, yaw_count, roll_count

user_idx = 3
average_df_all_img, max_df_all_img, min_df_all_img, std_df_all_img, var_df_all_img = calculate_statistics_in_mask_and_others(user_idx, 0.5)

average_df_all_img.head(-1)

x_count, y_count, z_count, pitch_count, yaw_count, roll_count = count_compaired(average_df_all_img)
x_count, y_count, z_count, pitch_count, yaw_count, roll_count = count_compaired(std_df_all_img)
x_count, y_count, z_count, pitch_count, yaw_count, roll_count = count_compaired(var_df_all_img)

feature_list = ["x_ave", "y_ave", "z_ave", "pitch_ave", "yaw_ave", "roll_ave", 
                "x_var", "y_var", "z_var", "pitch_var", "yaw_var", "roll_var",
                "x_max", "y_max", "z_max", "pitch_max", "yaw_max", "roll_max",
                "x_min", "y_min", "z_min", "pitch_min", "yaw_min", "roll_min"]
#all_user_ratio_df = pd.DataFrame({"features": feature_list})

for user_idx in range(0, len(user_list)):
    ratio_list = []
    # マスク領域とそれ以外での各画像の統計量算出
    average_df_all_img, max_df_all_img, min_df_all_img, std_df_all_img, var_df_all_img = calculate_statistics_in_mask_and_others(user_idx, 0.5)

    # 統計量(平均・分散)でマスク領域とそれ以外で比較
    # マスク領域の統計量が大きかったものを記録
    x_ave, y_ave, z_ave, pitch_ave, yaw_ave, roll_ave = count_compaired(average_df_all_img)
    x_var, y_var, z_var, pitch_var, yaw_var, roll_var = count_compaired(var_df_all_img)
    x_max, y_max, z_max, pitch_max, yaw_max, roll_max = count_compaired(max_df_all_img)
    x_min, y_min, z_min, pitch_min, yaw_min, roll_min = count_compaired(min_df_all_img)

    # 平均
    ratio_list.append(x_ave.sum()/len(x_ave))
    ratio_list.append(y_ave.sum()/len(y_ave))
    ratio_list.append(z_ave.sum()/len(z_ave))
    ratio_list.append(pitch_ave.sum()/len(pitch_ave))
    ratio_list.append(yaw_ave.sum()/len(yaw_ave))
    ratio_list.append(roll_ave.sum()/len(roll_ave))
    # 分散
    ratio_list.append(x_var.sum()/len(x_var))
    ratio_list.append(y_var.sum()/len(y_var))
    ratio_list.append(z_var.sum()/len(z_var))
    ratio_list.append(pitch_var.sum()/len(pitch_var))
    ratio_list.append(yaw_var.sum()/len(yaw_var))
    ratio_list.append(roll_var.sum()/len(roll_var))
    # 最大
    ratio_list.append(x_max.sum()/len(x_max))
    ratio_list.append(y_max.sum()/len(y_max))
    ratio_list.append(z_max.sum()/len(z_max))
    ratio_list.append(pitch_max.sum()/len(pitch_max))
    ratio_list.append(yaw_max.sum()/len(yaw_max))
    ratio_list.append(roll_max.sum()/len(roll_max))

    # 最小
    ratio_list.append(x_min.sum()/len(x_min))
    ratio_list.append(y_min.sum()/len(y_min))
    ratio_list.append(z_min.sum()/len(z_min))
    ratio_list.append(pitch_min.sum()/len(pitch_min))
    ratio_list.append(yaw_min.sum()/len(yaw_min))
    ratio_list.append(roll_min.sum()/len(roll_min))


    # DataFrameの更新
    ratio_df = pd.DataFrame(ratio_list, index = feature_list, columns = [str(user_idx)])
    if user_idx == 0:
        all_user_ratio_df = ratio_df
    else:
        all_user_ratio_df = pd.concat([all_user_ratio_df, ratio_df], axis = 1)

# 0の値はマスクなしの領域
print(all_user_ratio_df)

all_user_ratio_df_modified = all_user_ratio_df
# 絶対値(値 - 0.5) : 0.5が最も中途半端な割合なので、そこから離れているものを探す
all_user_ratio_df_modified = abs(all_user_ratio_df_modified - 0.5)
# (マスクなし設定)を0に変換
all_user_ratio_df_modified = all_user_ratio_df_modified.where(all_user_ratio_df_modified < 0.5, 0)
print(all_user_ratio_df_modified)

# 一定の割合のもののみを抜粋
key = all_user_ratio_df_modified >= 0.3

print(all_user_ratio_df_modified.iloc[:, 6])